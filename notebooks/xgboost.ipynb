{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGBoost\n",
    "\n",
    "<strong>Disclaimer</strong> -- this notebook was used for quick tests, code may not be functioning or sequential with different parameters. Please see the combined notebook for the final process used with xgboost. This file will be included as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data \n",
    "properties = pd.read_csv('../input/properties_2016.csv')\n",
    "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n",
    "\n",
    "\n",
    "# data pre-processing and train/test\n",
    "for column in properties.columns:\n",
    "    properties[column] = properties[column].fillna(-1)\n",
    "    if properties[column].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[column].values))\n",
    "        properties[column] = lbl.transform(list(properties[column].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid','decktypeid', 'yardbuildingsqft26', 'basementsqft',\n",
    "                        'buildingclasstypeid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outliers removal\n",
    "train_df = train_df[train_df.logerror > -0.4]\n",
    "train_df = train_df[train_df.logerror < 0.419]\n",
    "\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate',\n",
    "                        'decktypeid', 'yardbuildingsqft26', 'basementsqft',\n",
    "                        'buildingclasstypeid'], axis=1)\n",
    "\n",
    "y_train = train_df['logerror'].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('x_train: ', x_train.shape)\n",
    "print('x_test: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search + Cross Validation\n",
    "\n",
    "Note: Test metrics to get model working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5,7], \n",
    "             'min_child_weight': [1,3,5]}\n",
    "\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBRegressor(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn syntax \n",
    "optimized_GBM.fit(x_train, y_train)\n",
    "#xgb.train(xgb_params, dtrain, num_boost_round=5000, nfold=5, metrics=['mae'],\n",
    "                   #early_stopping_rounds=100, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "for eta in [0.030, 0.032, 0.034, 0.036, 0.038]:\n",
    "    print(eta)\n",
    "    for lamb_da in [0.6, 0.7, 0.8, 0.9]:\n",
    "        print(lamb_da)\n",
    "        xgb_params = {\n",
    "            'eta': eta,\n",
    "            'max_depth': int(5),\n",
    "            'subsample': 0.80,\n",
    "            'objective': 'reg:linear',\n",
    "            'eval_metric': 'mae',\n",
    "            'lambda': lamb_da,   \n",
    "            'alpha': 0.4, \n",
    "            'base_score': y_mean,\n",
    "            'silent': 1\n",
    "            }\n",
    "        # each combination -- run single model\n",
    "        print('Fitting model...')\n",
    "        print('ETA = ', eta, '\\nLambda = ', lamb_da)\n",
    "        # perform cross validation\n",
    "        scores = xgb.cv(xgb_params, dtrain, num_boost_round=5000, nfold=5, metrics=['mae'],\n",
    "                   early_stopping_rounds=100, stratified=True)\n",
    "        # compute mean cv accuracy\n",
    "        score = np.mean(scores)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'eta': eta, 'lamda': lamb_da}\n",
    "            \n",
    "# build final model w/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': int(5),\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "num_boost_rounds = 240\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "# second run\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "xgb_pred = (xgb_pred1 + xgb_pred2) / 2\n",
    "\n",
    "del train_df; gc.collect()\n",
    "del x_train; gc.collect()\n",
    "del x_test; gc.collect()\n",
    "del properties; gc.collect()\n",
    "del dtest; gc.collect()\n",
    "del dtrain; gc.collect()\n",
    "del xgb_pred1, xgb_pred2; gc.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n",
    "test_columns = ['201610','201611','201612','201710','201711','201712']\n",
    "\n",
    "sub = pd.read_csv('../submissions/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = xgb_pred\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('xgb_rm5features.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Msc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAE(y, y_pred):\n",
    "    return np.sum([abs(y[i] - y_pred[i]) for i in range(len(y))]) / len(y)\n",
    "# x_train, y_train\n",
    "print(MAE(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n",
    "test_columns = ['201610','201611','201612','201710','201711','201712']\n",
    "\n",
    "sub = pd.read_csv('../submissions/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('xgb_cv_v2.csv', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
